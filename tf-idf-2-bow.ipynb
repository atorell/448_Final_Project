{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1554b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#          /scratch/eecs448w24_class_root/eecs448w24_class/shared_data/valr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb4b44",
   "metadata": {},
   "source": [
    "# Libraries and Data Read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387a34d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atorell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/atorell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/atorell/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Linear regression Question libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Logistic Regression Question Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f65c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = '/scratch/eecs448w24_class_root/eecs448w24_class/shared_data/valr/Combined_Goodreads_Data.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fd1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  book_id                                              title  \\\n",
      "0           1        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "1           2        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "2           3        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "3           4        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "4           5        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "5           6        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "6           7        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "7           8        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "8           9        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "9          10        1  Harry Potter and the Half-Blood Prince (Harry ...   \n",
      "\n",
      "                      authors  average_rating        isbn         isbn13  \\\n",
      "0  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "1  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "2  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "3  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "4  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "5  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "6  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "7  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "8  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "9  J.K. Rowling/Mary GrandPré            4.57  0439785960  9780439785969   \n",
      "\n",
      "  language_code  num_pages  ratings_count  ...  \\\n",
      "0           eng        652        2095690  ...   \n",
      "1           eng        652        2095690  ...   \n",
      "2           eng        652        2095690  ...   \n",
      "3           eng        652        2095690  ...   \n",
      "4           eng        652        2095690  ...   \n",
      "5           eng        652        2095690  ...   \n",
      "6           eng        652        2095690  ...   \n",
      "7           eng        652        2095690  ...   \n",
      "8           eng        652        2095690  ...   \n",
      "9           eng        652        2095690  ...   \n",
      "\n",
      "                            user_id                         review_id rating  \\\n",
      "0  9435de79affccec8f57467984ce47ba2  ff1e2f99e65d0d288843a245ce213b78      5   \n",
      "1  7c4786a00b61cca2567367fa842f798d  cea77e2298bc8ccc5f21c94da570060d      3   \n",
      "2  e615077221d975b624a49ec1c03c1006  c49d484704c8d5c76c17d5e4239881d4      5   \n",
      "3  849604d6ffababef985f4e634e6c34c7  4035bbcc9a16228c4e41071a2fad338f      5   \n",
      "4  04b7d8bb5fd1451e976675fd9db7d841  76a3d09330cf71a6a578ba17dd8b1317      5   \n",
      "5  f115dc148ad8536b6fbc38abf48da61e  cfe09092cbfabc5011a5c08664979dbc      5   \n",
      "6  6ae2351130d8957b27f32870850a9536  ca4d97a11c67d8493fc2636af82d9a16      5   \n",
      "7  6afe25e4a5efe2568d223dcba6b72d9f  3899b801f16175f96b4284e0daaf4774      5   \n",
      "8  f9f5fe32aef71f81257dd07de9689e84  71d62b081e5c699b4c3bf994f8b0f921      5   \n",
      "9  4c89a84423deaf5dc31a532ef64dc9d1  e8b1b9372df4e19ba90d915b5837e337      5   \n",
      "\n",
      "                                         review_text  \\\n",
      "0  Ugh the feels. I was hit with them all over ag...   \n",
      "1  I liked it more than Harry Potter 4 and 5. Har...   \n",
      "2  Re-reading this book for the first time since ...   \n",
      "3  Obviously a re-read, this time in english. Loo...   \n",
      "4  I grew up reading HP and I have always loved i...   \n",
      "5  I watched the interview with Jim Dale in which...   \n",
      "6  This is both the best and worst HP so far! I l...   \n",
      "7  What did I do to deserve this. \\n I'm blinded ...   \n",
      "8  This series only gets better as you reread it,...   \n",
      "9  I liked it, but it wasn't as exiting as the ot...   \n",
      "\n",
      "                       date_added                    date_updated  \\\n",
      "0  Sun Feb 23 13:18:08 -0800 2014  Mon Jul 28 15:18:27 -0700 2014   \n",
      "1  Wed May 25 13:32:10 -0700 2016  Sat Nov 05 12:47:45 -0700 2016   \n",
      "2  Mon Jun 07 21:34:29 -0700 2010  Tue Apr 26 18:57:48 -0700 2016   \n",
      "3  Sun Aug 23 08:49:05 -0700 2015  Fri Jun 30 10:30:08 -0700 2017   \n",
      "4  Sat Oct 24 06:37:37 -0700 2009  Tue Apr 29 01:54:16 -0700 2014   \n",
      "5  Fri Nov 18 00:17:16 -0800 2016  Wed Nov 23 19:00:54 -0800 2016   \n",
      "6  Mon Mar 05 16:26:55 -0800 2012  Wed Apr 30 01:33:51 -0700 2014   \n",
      "7  Fri Jul 29 10:25:39 -0700 2016  Tue Aug 02 15:43:52 -0700 2016   \n",
      "8  Thu Jul 25 09:51:37 -0700 2013  Mon Jul 11 10:01:51 -0700 2016   \n",
      "9  Mon Oct 27 18:04:50 -0700 2014  Sun Nov 02 07:14:41 -0800 2014   \n",
      "\n",
      "                          read_at                      started_at n_votes  \\\n",
      "0  Mon Jul 28 15:18:27 -0700 2014  Fri Jul 25 00:00:00 -0700 2014       0   \n",
      "1  Sat Nov 05 16:26:06 -0700 2016  Sun Oct 16 00:00:00 -0700 2016       8   \n",
      "2  Tue Apr 26 00:00:00 -0700 2016  Fri Apr 15 00:00:00 -0700 2016       0   \n",
      "3  Tue Jun 13 00:00:00 -0700 2017  Mon May 22 00:00:00 -0700 2017       0   \n",
      "4  Tue Jan 01 00:00:00 -0800 2008                             NaN       0   \n",
      "5  Wed Nov 23 00:00:00 -0800 2016  Fri Nov 18 00:00:00 -0800 2016       1   \n",
      "6  Sun Mar 18 13:11:00 -0700 2012  Sun Mar 04 00:00:00 -0800 2012       0   \n",
      "7  Tue Aug 02 15:43:52 -0700 2016  Fri Jul 29 00:00:00 -0700 2016       1   \n",
      "8  Mon Jul 11 13:38:58 -0700 2016  Wed Jul 06 00:00:00 -0700 2016       1   \n",
      "9  Sun Nov 02 07:14:41 -0800 2014  Mon Oct 27 00:00:00 -0700 2014       0   \n",
      "\n",
      "  n_comments  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "5          0  \n",
      "6          0  \n",
      "7          0  \n",
      "8          0  \n",
      "9          0  \n",
      "\n",
      "[10 rows x 23 columns]\n",
      "0        9/16/2006\n",
      "1        9/16/2006\n",
      "2        9/16/2006\n",
      "3        9/16/2006\n",
      "4        9/16/2006\n",
      "           ...    \n",
      "54946     7/1/2004\n",
      "54947     7/1/2004\n",
      "54948     7/1/2004\n",
      "54949     7/1/2004\n",
      "54950     7/1/2004\n",
      "Name: publication_date, Length: 54951, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))\n",
    "print(df[\"publication_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fc002",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfe51e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54951, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 999 rows and 11 features\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4c445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54951 entries, 0 to 54950\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          54951 non-null  int64  \n",
      " 1   book_id             54951 non-null  int64  \n",
      " 2   title               54951 non-null  object \n",
      " 3   authors             54951 non-null  object \n",
      " 4   average_rating      54951 non-null  float64\n",
      " 5   isbn                54951 non-null  object \n",
      " 6   isbn13              54951 non-null  int64  \n",
      " 7   language_code       54951 non-null  object \n",
      " 8   num_pages           54951 non-null  int64  \n",
      " 9   ratings_count       54951 non-null  int64  \n",
      " 10  text_reviews_count  54951 non-null  int64  \n",
      " 11  publication_date    54951 non-null  object \n",
      " 12  publisher           54951 non-null  object \n",
      " 13  user_id             54951 non-null  object \n",
      " 14  review_id           54951 non-null  object \n",
      " 15  rating              54951 non-null  int64  \n",
      " 16  review_text         54951 non-null  object \n",
      " 17  date_added          54951 non-null  object \n",
      " 18  date_updated        54951 non-null  object \n",
      " 19  read_at             45609 non-null  object \n",
      " 20  started_at          29555 non-null  object \n",
      " 21  n_votes             54951 non-null  int64  \n",
      " 22  n_comments          54951 non-null  int64  \n",
      "dtypes: float64(1), int64(9), object(13)\n",
      "memory usage: 9.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                0\n",
       "book_id                   0\n",
       "title                     0\n",
       "authors                   0\n",
       "average_rating            0\n",
       "isbn                      0\n",
       "isbn13                    0\n",
       "language_code             0\n",
       "num_pages                 0\n",
       "ratings_count             0\n",
       "text_reviews_count        0\n",
       "publication_date          0\n",
       "publisher                 0\n",
       "user_id                   0\n",
       "review_id                 0\n",
       "rating                    0\n",
       "review_text               0\n",
       "date_added                0\n",
       "date_updated              0\n",
       "read_at                9342\n",
       "started_at            25396\n",
       "n_votes                   0\n",
       "n_comments                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 features are numerical and the rest are strings \n",
    "# 2 features have null rows \n",
    "# we can also see the column names \n",
    "df.info()\n",
    "\n",
    "# because we know we have some missing values:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa6ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    " \n",
    "sns.pairplot(df)\n",
    " \n",
    "plt.suptitle('Pair Plot for DataFrame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the rating are higher, there are more votes \n",
    "ratings = df['rating'].value_counts()\n",
    "plt.bar(ratings.index, ratings)\n",
    "plt.title('Count Plot of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "#sns.boxplot(x=\"rating\", y=\"n_votes\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa57ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there are higher ratings, there are more comments. But this is not as significant as votes. \n",
    "# comments = df['n_comments'].value_counts()\n",
    "# plt.bar(comments.index, df['rating'])\n",
    "# sns.boxplot(x=\"rating\", y=\"n_comments\", data=df)\n",
    "comments_per_rating = df.groupby('rating')['n_comments'].sum()\n",
    "\n",
    "# Plot the number of comments per rating category\n",
    "plt.bar(comments_per_rating.index, comments_per_rating.values, color =\"orange\")\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Number of Comments per Rating Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50301400",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_per_rating = df.groupby('rating')['n_votes'].sum()\n",
    "\n",
    "# Plot the number of comments per rating category\n",
    "plt.bar(votes_per_rating.index, votes_per_rating.values, color=\"magenta\")\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Votes')\n",
    "plt.title('Number of Votes per Rating Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32598dc1",
   "metadata": {},
   "source": [
    "Simple exploratory data analysis is helpful but not exptremely helpful for our dataset because there are a lot of qualitative variables including our feature of interest, reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611635ee",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3235e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def PreprocessSentence(sentence, nltk_stopwords):\n",
    "    # Input - a sentence in the form of a python string, and the set of stopwords\n",
    "    # Output - the preprocessed sentence, as per the instructions\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    #remove stopwords and single char non-num words\n",
    "    tokens = [word for word in tokens if word not in nltk_stopwords and (word.isnumeric() or len(word) > 1)]\n",
    "    preprocessed_sentence = ' '.join(tokens)\n",
    "    return preprocessed_sentence\n",
    "\n",
    "def PreprocessData(df, nltk_stopwords):\n",
    "    # Input - the dataframe, with columns label and text, and the set of stopwords\n",
    "    # output - the dataframe with the text processed as described earlier\n",
    "    df['review_text'] = df['review_text'].map(lambda x: PreprocessSentence(x, nltk_stopwords))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5cdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PreprocessData(df, nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56cacca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        3\n",
       "2        5\n",
       "3        5\n",
       "4        5\n",
       "        ..\n",
       "54946    4\n",
       "54947    3\n",
       "54948    3\n",
       "54949    3\n",
       "54950    3\n",
       "Name: rating, Length: 54951, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a92a75",
   "metadata": {},
   "source": [
    "# Basic Random Model Based on Overall Distribution of Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814b46e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1246  1887  4048  9757 14527 12495]\n",
      "Random Model Accuracy: 0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.02      0.02       311\n",
      "           1       0.03      0.03      0.03       480\n",
      "           2       0.10      0.10      0.10      1049\n",
      "           3       0.22      0.23      0.23      2434\n",
      "           4       0.32      0.32      0.32      3610\n",
      "           5       0.28      0.28      0.28      3107\n",
      "\n",
      "    accuracy                           0.25     10991\n",
      "   macro avg       0.16      0.16      0.16     10991\n",
      "weighted avg       0.24      0.25      0.24     10991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "np.random.seed(80)\n",
    "\n",
    "X = df['review_text']\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "distribution = np.bincount(y_train)\n",
    "print(distribution)\n",
    "\n",
    "y_pred = np.random.choice(len(distribution), size=len(X_test), p=distribution/len(y_train))\n",
    "\n",
    "accuracy_random = accuracy_score(y_test, y_pred)\n",
    "print(f'Random Model Accuracy: {accuracy_random:.2f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad0908",
   "metadata": {},
   "source": [
    "# Bag of Words/N-grams\n",
    "Code adapted from: https://towardsdatascience.com/leveraging-n-grams-to-extract-context-from-text-bdc576b47049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea777dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to all these imports in this cell because it oddly wasn't working if it was imported in an earlier cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# define X and y data\n",
    "X = df['review_text']\n",
    "y = df['rating']\n",
    "\n",
    "\n",
    "# initialize count vectorizer\n",
    "bow_cv = CountVectorizer()\n",
    "\n",
    "# create document-term matrix\n",
    "X_cv = bow_cv.fit_transform(X)\n",
    "\n",
    "# classify with logistic regression\n",
    "\n",
    "np.random.seed(80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size=0.2)\n",
    "# X_train = X_train.toarray()\n",
    "# X_test = X_test.toarray()\n",
    "# # Instantiate the logistic regression model\n",
    "# #logistic_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "# logistic_model = GaussianNB()\n",
    "\n",
    "# # Fit it to the training data\n",
    "# logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# # We can create a set of predictions on the test dataset\n",
    "# predictions = logistic_model.predict(X_test)\n",
    "\n",
    "# # Scikit lets you directly compute the score on a dataset using the model's score method\n",
    "# train_score = logistic_model.score(X_train, y_train)\n",
    "# test_score = logistic_model.score(X_test, y_test)\n",
    "\n",
    "# # Now lets view our scores!\n",
    "# print(\"Train Accuracy:\", round(train_score*100,2), \"%\")\n",
    "# print(\"Test Accuracy:\", round(test_score*100,2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d006602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the accuracies\n",
    "train_accuracy = 83.96\n",
    "test_accuracy = 41.45\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['lightgreen', 'gold'])\n",
    "plt.ylim(0, 100)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies')\n",
    "\n",
    "# Annotate the bar chart with accuracy values\n",
    "plt.text(0, train_accuracy, f'{train_accuracy}%', ha='center', va='bottom', fontsize=12)\n",
    "plt.text(1, test_accuracy, f'{test_accuracy}%', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Ratings\n",
    "plt.hist(y, bins=6, alpha=0.7)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "# feature_names = bow_cv.get_feature_names()\n",
    "# coefficients = logistic_model.coef_[0]\n",
    "# top_features = sorted(zip(coefficients, feature_names), reverse=True)[:20]\n",
    "# plt.barh([x[1] for x in top_features], [x[0] for x in top_features])\n",
    "# plt.xlabel('Coefficient')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.title('Top 20 Features by Coefficient')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4188cc5",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e13eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract the preprocessed \"review_text\" column\n",
    "reviews = df['review_text']\n",
    "\n",
    "# Step 3: Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 4: Fit and transform the preprocessed review text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Optional: If you want to see the list of features (words) extracted by TF-IDF\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cb4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df['review_text'])\n",
    "    y_labels = df['rating'].to_numpy()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tfidf = pd.DataFrame(data=X.toarray(), columns=feature_names)\n",
    "    x_array = X.toarray()\n",
    "    return X, y_labels, vectorizer\n",
    "\n",
    "# Step 2: Apply the vectorize function to transform the preprocessed reviews into TF-IDF vectors\n",
    "X_tfidf, y_labels, vectorizer = vectorize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_tfidf, y_labels, test_size=0.2)\n",
    "\n",
    "# Use the vectorizer to create our X and y data\n",
    "# X, y, vectorizer = vectorize(df)\n",
    "\n",
    "#X_tfidf, y_labels, vectorizer = vectorize(df)\n",
    "\n",
    "#X_tfidf will contain the TF-IDF representation of your preprocessed \"review_text\" column,\n",
    "#y_labels will contain the corresponding labels,\n",
    "#and vectorizer will be the TF-IDF vectorizer used for transformation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_array = X_tfidf.toarray()\n",
    "# print(x_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c33843",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "830d1a0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 64.43 %\n",
      "Test Accuracy: 43.37 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "np.random.seed(80)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_labels, test_size=0.2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Fit it to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# We can create a set of predictions on the test dataset\n",
    "predictions = logistic_model.predict(X_test)\n",
    "\n",
    "# Scikit lets you directly compute the score on a dataset using the model's score method\n",
    "train_score = logistic_model.score(X_train, y_train)\n",
    "test_score = logistic_model.score(X_test, y_test)\n",
    "\n",
    "# Now lets view our scores!\n",
    "print(\"Train Accuracy:\", round(train_score*100,2), \"%\")\n",
    "print(\"Test Accuracy:\", round(test_score*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the accuracies\n",
    "train_accuracy = 64.43\n",
    "test_accuracy = 43.36\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['aqua', 'coral'])\n",
    "plt.ylim(0, 100)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies')\n",
    "\n",
    "# Annotate the bar chart with accuracy values\n",
    "plt.text(0, train_accuracy, f'{train_accuracy}%', ha='center', va='bottom', fontsize=12)\n",
    "plt.text(1, test_accuracy, f'{test_accuracy}%', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b769c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc575cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store train and test accuracies\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Loop over a range of test sizes or any other variations you want to test\n",
    "test_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_labels, test_size=test_size)\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    train_accuracies.append(logistic_model.score(X_train, y_train))\n",
    "    test_accuracies.append(logistic_model.score(X_test, y_test))\n",
    "\n",
    "# Plotting train and test accuracies\n",
    "plt.plot(test_sizes, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_sizes, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracies vs Test Size')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44195201",
   "metadata": {},
   "source": [
    "# TF-IDF with Cross-Validation, Multinomial Logistic Regression Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55116c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(80)\n",
    "\n",
    "X = df['review_text']\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32babce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "np.random.seed(80)\n",
    "model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-Validation Scores:\", cross_val_scores)\n",
    "print(\"Mean Validation Accuracy:\", cross_val_scores.mean())\n",
    "\n",
    "#fit the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#predict\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print a classification report for more detailed evaluation\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a92b5b",
   "metadata": {},
   "source": [
    "# TF-IDF with Dense Neural Network, not currently working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b398065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train, X_val, y_train, y_val are your training and validation sets\n",
    "# X_train and X_val are the text data, y_train and y_val are the ratings\n",
    "\n",
    "# Step 1: Vectorize the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 2: Build a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train_tfidf.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))  # Assuming 5 ratings (1 to 5)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the neural network on the training data\n",
    "model.fit(X_train_tfidf.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Step 4: Evaluate the model on the validation data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n",
    "\n",
    "# Step 5: Predict on the validation data\n",
    "y_test_pred = model.predict_classes(X_test_tfidf.toarray())\n",
    "\n",
    "# Print a classification report for more detailed evaluation\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7633a7",
   "metadata": {},
   "source": [
    "# Attempt at sentiment analysis with GaussianMixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "np.random.seed(80)\n",
    "\n",
    "X = df['review_text']\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "sentiments = []\n",
    "sentiments.append([])\n",
    "sentiments.append([])\n",
    "sentiments.append([])\n",
    "sentiments.append([])\n",
    "sentiments.append([])\n",
    "sentiments.append([])\n",
    "\n",
    "\n",
    "#create lists of sentiment scores by rating\n",
    "for i in range(len(X_train)):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    compound_score = sia.polarity_scores(X_train.iloc[i])['compound']\n",
    "    sentiments[y_train.iloc[i]].append([compound_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd322b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create guassianmixtures\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "np.random.seed(80)\n",
    "\n",
    "covs = ['diag', 'full']\n",
    "components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "accuracies = []\n",
    "\n",
    "for component in components:\n",
    "    for cov in covs:\n",
    "        models = []\n",
    "\n",
    "        for ratings in sentiments:\n",
    "            gmm = GaussianMixture(n_components=7, covariance_type='diag', n_init=3)\n",
    "            gmm.fit(ratings)\n",
    "            models.append(gmm)\n",
    "\n",
    "        #predict train scores\n",
    "\n",
    "\n",
    "        y_pred = []\n",
    "        for review in X_test:\n",
    "            scores = []\n",
    "            sia = SentimentIntensityAnalyzer()\n",
    "            compound_score = sia.polarity_scores(review)['compound']\n",
    "            for m in models:\n",
    "                score = m.score([[compound_score]])\n",
    "                scores.append(score)\n",
    "            y_pred.append(np.argmax(scores))\n",
    "\n",
    "\n",
    "        #calculate accuracy\n",
    "\n",
    "        total = len(y_test)\n",
    "        correct = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_pred[i] == y_test.iloc[i]:\n",
    "                correct = correct + 1\n",
    "        test_accuracy = float(correct)/float(total)\n",
    "        print(\"Test Accuracy with covariance: \", cov, \" and \", component, \" components: \", round(test_accuracy*100,2), \"%\")\n",
    "        accuracies.append([test_accuracy, cov, component])\n",
    "        \n",
    "best_acc = 0\n",
    "best_cov = \"\"\n",
    "best_comp = 0\n",
    "for accuracy in accuracies:\n",
    "    if accuracy[0] > best_acc:\n",
    "        best_acc = accuracy[0]\n",
    "        best_cov = accuracy[1]\n",
    "        best_comp = accuracy[2]\n",
    "print(\"Best Test Accuracy with covariance: \", best_cov, \" and \", best_comp, \" components: \", round(best_acc*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37483045",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116af4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# attempting multiclass classification\n",
    "# one-versus-one approach\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size=0.2)\n",
    "# X = [X_train]\n",
    "# Y = [0,1,2,3,4,5]\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Decision function shape (before change):\", clf.decision_function([[1]])) # Output should be the shape of ovo\n",
    "\n",
    "# Change decision function shape\n",
    "clf.decision_function_shape = \"ovr\"\n",
    "\n",
    "# Check decision function shape again\n",
    "print(\"Decision function shape (after change):\", clf.decision_function([[1]])) # Output should be the shape of ovr\n",
    "# SVC(decision_function_shape='ovo')\n",
    "# dec = clf.decision_function([[1]])\n",
    "# dec.shape[1] # 6 classes: 6*3/2 = 9\n",
    "# clf.decision_function_shape = \"ovr\"\n",
    "# dec = clf.decision_function([[1]])\n",
    "# dec.shape[1] # 6 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13a86d",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6c92c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3893699/4068539064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'review_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorize' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tfidf, y_labels, vectorizer = vectorize(df)\n",
    "X_original = df.drop(['rating', 'review_text'])\n",
    "df2 = X_original.assign(X_tfidf=X_tfidf)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2, y_labels, test_size=0.2)\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train[\"review_text\"])\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# X_tfidf, y_labels, vectorizer = vectorize(df)\n",
    "\n",
    "# # Convert X_tfidf to a DataFrame\n",
    "# X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "\n",
    "# # Remove 'rating' and 'review_text' columns from df\n",
    "# X_original = df.drop(['rating', 'review_text'])\n",
    "\n",
    "# # Concatenate X_tfidf_df with X_original\n",
    "# X_all = pd.concat([X_original, X_tfidf_df], axis=1)\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_all, y_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_count = 6\n",
    "km = KMeans(n_clusters = cluster_count, # The number of clusters we want\n",
    "            n_init = 10, # This is the number of time the algorithm runs - the result is the one with minimum SSE (sum of squared errors)\n",
    "            init='random', # How the centroids are initialized\n",
    "            max_iter = 100,\n",
    "            random_state = 42)\n",
    "\n",
    "preds = km.fit_predict(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14dd4acc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4159140/4288155704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mplotClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'lightgreen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lightblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pink'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yellow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orange'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4159140/4288155704.py\u001b[0m in \u001b[0;36mplotClusters\u001b[0;34m(X_train_tfidf, preds, km, cluster_count, colorlist)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplotClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolorlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Convert sparse matrix to dense array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         plt.scatter(X_train_dense[preds == i, 0],\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "# theres something wrong with this \n",
    "\n",
    "def plotClusters(X_train_tfidf, preds, km, cluster_count, colorlist):\n",
    "    # Convert sparse matrix to dense array\n",
    "    X_train_dense = X_train.toarray()\n",
    "    for i in range(cluster_count):\n",
    "        plt.scatter(X_train_dense[preds == i, 0],\n",
    "                    X_train_dense[preds == i, 1],\n",
    "                    marker=\"o\",\n",
    "                    c=colorlist[i],\n",
    "                    label=\"Cluster \" + str(i + 1))\n",
    "    # plot the centroids\n",
    "    plt.scatter(km.cluster_centers_[:, 0],\n",
    "                km.cluster_centers_[:, 1],\n",
    "                s=250,\n",
    "                marker='*',\n",
    "                c='red',\n",
    "                edgecolor='black',\n",
    "                label='Centroids')\n",
    "    plt.legend(scatterpoints=1)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "        \n",
    "plotClusters(X_train_tfidf, preds, km, cluster_count, ['lightgreen', 'lightblue', 'blue', 'pink', 'yellow', 'orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate within-cluster SSE varying the number of clusters\n",
    "# The elbow graph will show a sharp decrease in the intra-cluster SSE till a certain point, after which the reduction is slow\n",
    "# This elbow point is the optimal number of clusters\n",
    "\n",
    "cluster_sse = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='random',\n",
    "                n_init=10, \n",
    "                max_iter=100,\n",
    "                random_state=42)\n",
    "    km.fit(X_train)\n",
    "    cluster_sse.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), cluster_sse, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Intra-Cluster SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = 2\n",
    "km = KMeans(n_clusters = cluster_count, # The number of clusters we want\n",
    "            n_init = 10, # This is the number of time the algorithm runs - the result is the one with minimum SSE (sum of squared errors)\n",
    "            init='random', # How the centroids are initialized\n",
    "            max_iter = 100,\n",
    "            random_state = 42)\n",
    "\n",
    "preds = km.fit_predict(X_train)\n",
    "\n",
    "plotClusters(X_train, preds, km, cluster_count, ['lightgreen', 'lightblue', 'blue', 'pink', 'yellow', 'orange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532f236",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5e8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4e2be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4159140/2502292530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n\u001b[0;32m----> 2\u001b[0;31m     max_depth=1, max_features = 21, random_state=0).fit(X_train, y_train)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, max_features = 21, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],       # Maximum depth of the trees\n",
    "    'learning_rate': [0.01, 0.2, 0.5],   # Minimum number of samples required to split an internal node\n",
    "    'random_state': [0, 50, 75]      # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters found by grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy with Best Model:\", round(test_score * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e900a",
   "metadata": {},
   "source": [
    "# Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ryan Galligan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05e31236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.67 %\n",
      "Test Accuracy: 42.42 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_tfidf, y_labels, test_size=0.2)\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit it to the training data\n",
    "random_forest_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Predictions on the test dataset\n",
    "predictions = random_forest_model.predict(X_test_rf)\n",
    "\n",
    "# Compute the accuracy scores\n",
    "train_score = random_forest_model.score(X_train_rf, y_train_rf)\n",
    "test_score = random_forest_model.score(X_test_rf, y_test_rf)\n",
    "\n",
    "# Output the scores\n",
    "print(\"Train Accuracy:\", round(train_score * 100, 2), \"%\")\n",
    "print(\"Test Accuracy:\", round(test_score * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e3994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 150}\n",
      "Test Accuracy with Best Model: 42.64 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "#     'max_depth': [None, 10, 20],       # Maximum depth of the trees\n",
    "#     'min_samples_split': [2, 5, 10],   # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4]      # Minimum number of samples required to be at a leaf node\n",
    "# }\n",
    "\n",
    "# Define a simpler parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [12, 25, 50, 100, 150, 200],       # Fewer trees to reduce complexity\n",
    "    'max_depth': [None, 2, 5, 8, 15, 20, 25],        # No limit on depth\n",
    "    'min_samples_split': [2, 5, 10, 15],   # Minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2, 4]     # Keep it simple with only one value\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit grid search to the training data\n",
    "grid_search.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Get the best hyperparameters found by grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_score = best_model.score(X_test_rf, y_test_rf)\n",
    "print(\"Test Accuracy with Best Model:\", round(test_score * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "debc8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Test Accuracy with Best Model: 42.55 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],        # Reducing the number of trees\n",
    "    'max_depth': [None, 10],          # Limiting the maximum depth\n",
    "    'min_samples_split': [5, 10],     # Increasing min_samples_split\n",
    "    'min_samples_leaf': [1, 2]        # Adjusting min_samples_leaf\n",
    "}\n",
    "\n",
    "# # Instantiate the Random Forest classifier\n",
    "# random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Perform grid search with 5-fold cross-validation\n",
    "# grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Fit grid search to the training data\n",
    "# grid_search.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Get the best hyperparameters found by grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the results of cross-validation\n",
    "cv_results = grid_search.cv_results_\n",
    "    \n",
    "# Get the best model found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_score = best_model.score(X_test_rf, y_test_rf)\n",
    "print(\"Test Accuracy with Best Model:\", round(test_score * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8da5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 15, 'n_estimators': 50}\n",
      "Test Accuracy with Best Model: 36.39 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a simpler parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [12, 25, 50, 75],       # Fewer trees to reduce complexity\n",
    "    'max_depth': [2, 5, 8, 15],        # No limit on depth\n",
    "    'min_samples_split': [5, 10, 15],   # Minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2]     # Keep it simple with only one value\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit grid search to the training data\n",
    "grid_search.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Get the best hyperparameters found by grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_score = best_model.score(X_test_rf, y_test_rf)\n",
    "print(\"Test Accuracy with Best Model:\", round(test_score * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1450d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 42.64 %\n"
     ]
    }
   ],
   "source": [
    "##BEST MODEL\n",
    "#Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 150}\n",
    "#Test Accuracy with Best Model: 42.64 %\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate the Random Forest classifier with specific hyperparameters\n",
    "random_forest_model = RandomForestClassifier(n_estimators=150, max_depth=None, min_samples_split=10, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "random_forest_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_score = random_forest_model.score(X_test_rf, y_test_rf)\n",
    "print(\"Test Accuracy:\", round(test_score * 100, 2), \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
