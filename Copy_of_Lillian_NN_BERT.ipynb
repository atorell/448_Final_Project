{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmQwE3Kb5-eL",
        "outputId": "a7fb2dfb-e102-4a92-d3eb-97220c7e317b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Linear regression Question libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "# Logistic Regression Question Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import torch\n",
        "torch.manual_seed(448)\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# from imblearn.over_sampling import RandomUnderSampler\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V09YL6GT6I_L"
      },
      "source": [
        "### Connect with Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftj5RIW96FzA",
        "outputId": "d4d2caf1-ec88-44bf-ec51-85921ee907c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sqZ3ycg6Sg_",
        "outputId": "24eef284-89ad-4a5a-b75e-04ce488c575b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Combined_Goodreads_Data.csv', 'copy of Anna_NN_BERT.ipynb', 'EECS 448 final NN.ipynb', 'Vidhur_NN_BERT.ipynb', 'Copy of Lillian_NN_BERT.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# TODO: change this to the path to your homework folder\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'EECS_448_Final_NN/'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "# Load the autoreload extension\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSnJJrdP7LCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4d0098-28df-45bb-e68f-762d4825dbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Installations\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEnQtjtv7Q2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b20a2bd-c720-441f-a347-4541682d200b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QZZLK417nsi"
      },
      "source": [
        "### Getting the Data Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHVrz9cZ7nbo"
      },
      "outputs": [],
      "source": [
        "#file_path = '/Combined_Goodreads_Data.csv'\n",
        "df = pd.read_csv(GOOGLE_DRIVE_PATH + \"Combined_Goodreads_Data.csv\")\n",
        "# print(df.head(10))\n",
        "\n",
        "# df_filtered = df[df['n_votes'] > 0]\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(df_filtered['n_votes'], bins=5, range=(0, 80), color='skyblue', edgecolor='black')\n",
        "# plt.title('Distribution of n_votes (excluding 0)')\n",
        "# plt.xlabel('n_votes')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True)\n",
        "# plt.xlim(0, 100)  # Set x-axis limits\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQSOg2HI8pn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2f1f7a-bd6e-48b9-839a-700255e82bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "n_votes_bin\n",
            "0    7504\n",
            "4    2742\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Converting the dates\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Convert date_added column to datetime objects\n",
        "try:\n",
        "    df['date_added'] = pd.to_datetime(df['date_added'], utc=True)\n",
        "except ValueError as e:\n",
        "    print(\"Error converting date_added column:\", e)\n",
        "\n",
        "# Format datetime objects as desired\n",
        "if 'date_added' in df.columns:\n",
        "    df['formatted_date_added'] = df['date_added'].dt.strftime(\"%m/%d/%Y\")\n",
        "\n",
        "# print(df['formatted_date_added'])\n",
        "\n",
        "# Filter out instances where n_votes is 0\n",
        "df = df[df['n_votes'] != 0]\n",
        "print(type(df['n_votes']))\n",
        "# Define bins\n",
        "df['n_votes_bin'] = pd.qcut(df['n_votes'], q=6, labels=False , duplicates='drop')\n",
        "df = df[df['n_votes_bin'] != 1]\n",
        "df = df[df['n_votes_bin'] != 2]\n",
        "df = df[df['n_votes_bin'] != 3]\n",
        "\n",
        "#df = df.reset_index()\n",
        "# Display the value counts of the bins\n",
        "print(df['n_votes_bin'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFj6zhRC9f4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6386d962-9b64-4208-fcee-0bc39e5d0e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5        3716\n",
            "7        3604\n",
            "8        2504\n",
            "13       3335\n",
            "15       2921\n",
            "         ... \n",
            "54922    2610\n",
            "54924    3984\n",
            "54929    2997\n",
            "54930    2842\n",
            "54943    4378\n",
            "Name: time_difference_days, Length: 10246, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df['formatted_date_added'] = pd.to_datetime(df['formatted_date_added'], utc=True)\n",
        "df['publication_date'] = pd.to_datetime(df['publication_date'])\n",
        "\n",
        "df['formatted_date_added'] = pd.to_datetime(df['formatted_date_added']).dt.tz_localize(None)\n",
        "\n",
        "# Calculate the time difference between publication_date and date_added\n",
        "df['time_difference_days'] = (df['formatted_date_added'] - df['publication_date']).dt.days\n",
        "\n",
        "print(df['time_difference_days'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m34rCP7C9moq"
      },
      "outputs": [],
      "source": [
        "df['review_word_count'] = df['review_text'].str.split().str.len()\n",
        "\n",
        "# print(df['review_word_count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UWWhhIM9q4W"
      },
      "outputs": [],
      "source": [
        "df['num_reviews'] = df.groupby('user_id')['user_id'].transform('size')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dQ9o4iR9u82"
      },
      "outputs": [],
      "source": [
        "df['num_book'] = df.groupby('book_id')['book_id'].transform('size')\n",
        "# print(df['num_book'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFkw5GwP9y9h"
      },
      "outputs": [],
      "source": [
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def PreprocessSentence(sentence, nltk_stopwords):\n",
        "    # Input - a sentence in the form of a python string, and the set of stopwords\n",
        "    # Output - the preprocessed sentence, as per the instructions\n",
        "\n",
        "    sentence = sentence.lower()\n",
        "    tokens = word_tokenize(sentence)\n",
        "    #remove stopwords and single char non-num words\n",
        "    tokens = [word for word in tokens if word not in nltk_stopwords and (word.isnumeric() or len(word) > 1)]\n",
        "    preprocessed_sentence = ' '.join(tokens)\n",
        "    return preprocessed_sentence\n",
        "\n",
        "def PreprocessData(df, nltk_stopwords):\n",
        "    # Input - the dataframe, with columns label and text, and the set of stopwords\n",
        "    # output - the dataframe with the text processed as described earlier\n",
        "    df['review_text'] = df['review_text'].map(lambda x: PreprocessSentence(x, nltk_stopwords))\n",
        "    return df\n",
        "df = PreprocessData(df, nltk_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_bert['n_votes_bin'].value_counts()"
      ],
      "metadata": {
        "id": "SGuGJA3wULyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcM8mb0AmtlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9eead60-d207-47d3-af87-6cc6a5bb93a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[\"3.5 hehehe like cabot 's stories 're laid back easy read high drama bubble hilariously along right amount romance took couple chapters really got reading easy sure might touch predictable n't care sometimes works yeah n't much say reading felt like break nice\"]\n",
            " [\"wonderful reading always zusak ca n't fail love style love love love stories marvellous book\"]\n",
            " [\"9 minutes past six evening writing review murder mystery novel like murder mystery novels involve puzzles try solve mystery answer revealed novel reviewing written boy asperger 's syndrome like reviews always know someone thought book reviewing number stars given novel top review social convention books marked five stars give liked book convention set agreed stipulated generally accepted standards liked book lot gave five stars first drew book title title long unusual novel title book 'the curious incident dog night-time says author mark haddon n't read book author actually christopher boone one mysteries contained within book states throughout writing book likes murder mystery novels well mystery needed solve later two mysteries solve initial mystery solve dog killed garden fork found 7 minutes past midnight dog belonged neighbour neighbour friend neigbour came accused christopher killing dog police officer touched christopher christopher hit think day must black day christopher due happened bus think would seen four yellow cars row read book however saw 5 red cars row super good day enjoyed book much also strawberry milkshake way home good christopher sets solve mystery killed dog garden fork proceeds investigate crime father 's wishes main premise book start develops wider story arc 'd like tell book friend says would make review full spoilers would diminish destroy value book read wo n't instead finish review get train home read another book write another review know christopher boone taught anything http //briefcasebooks.wordpress.com/2 ...\"]\n",
            " ...\n",
            " [\"life dull nothing said books talk would describe interesting obliged exaggerate hope justifying existence 've spent lot time thinking novel exactly 's exceedingly flexible fluid form novel accommodate historical behemoths like war peace philosophical exercises like brothers karamazov wacky experiments like ulysses mythical adventures like lord rings many works fiction call novels lack better term term encompass pale fire search lost time trial huckleberry finn shining road really characteristics multifarious works share 're fiction read think novels various novelish things perhaps ought name names useful reliably tell us something 're naming novel pure simple devoid hesitations qualifications well lieu simple definition 'd like offer gem book prime example mean passage india paragon novel form forster delicate craftsman without plodding descriptions obvious inner monologues manages convey personalities characters n't spill philosophy life rather commonplace conversations one another cadences speech turns thought banal interactions come know characters intimately -- pleasure forster 's characters wonderful works art ordinary interesting impressive imperfect likable limited short real people like great fictional characters also real people -- whole types people novelist simply capture people knows rather takes characteristic quality fits together like puzzle-pieces result person strikingly familiar yet strikingly new feel like 've met 're happy meet great novel drama feel manufactured contrived unlike cheap thriller excitement result improbable coincidences extraordinary events forster instead brings drama hidden everyday situations brings light tiny tensions daily anxieties result penetrating exploration human society psychology forster shows people shape shaped circumstances continuous interplay social environment personality unearths things never buried like great still-life forster 's novel n't much reveal remind great work science philosophy may make us cry `` interesting insightful '' great novel makes us cry `` true perfectly true '' forster 's novel things 's work art superlative example craft novel like good novel ca n't say without spoiling plot 'll see\"]\n",
            " [\"3.5 stars rounding quick pros complex story huge cast characters cool see people crossed paths love 's hard sf huge stakes 's easier spew cons pros one honestly really enjoyed story worldbuilding really need know ends quick cons um sexism yeah paula main female character n't get sex kitten moment probably 's written man wow hamilton 's portrayal women constantly using sex manipulate men gross 're sex position power 're `` ballcrusher '' n't want dig ... 's lot good god harems con sheer length hamilton describes lot stuff planetary geography city layouts every single thinv character etc excruciating detail 's really necessary like balance description actually moving story along less ponderous pace want finish story 'll checking judas unchained soon hopefully hamilton ditches harems sex nymphets angry men calling wives whores take verbosity ... perpetuated sexist slant much\"]\n",
            " [\"problem 1 writer answer grand families intersolar dynasties difference say idea eternal fight capitalism communism writer presents communism ultimate answer problem corrupt wealth course believed one intellectual characters book ...... story progresses get feel writer idiot constantly bring problems book discuses course reason writer produce eternal red menace anathema wealthy elite mind main character reason think writer really believes inherited rights elite posses.he n't believe freedom fighters sees terrorists n't believe common men could stand face unjust problem 2 body please tell writers write futuristic human community moral behavior humanity excessive interpretation new york la sex life could probably squeeze 90 western cities .... 400 years human life behavior evolution mean forgetting everything religion social tradition ... obey west views matter problem 3 common men book problem 4 religion book races muslims asians general obscure references india people remember god die sex place pleasure n't sin allot marriages though harems problem 5 900 pages description .of rich beautiful men women literally describes every man women book n't ugly average person course could reasonable explanation writer walks extra couple miles saying person pretty every single character despite marginal problem 6 writer spends approximately 250 pages describing painful detail sherlockian murder .and female sherlock detective solve crime twist would make sir arther conan doyle bloody proud would one write mystery novel middle space-opera novel problem 7 excessive description describes every thing ground .the train station every train station trains train engines seats river bed every river bed snow yeah faces noses eyes .............. maybe 'm wrong\"]]\n",
            "5484\n"
          ]
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "x_reshape = df['review_text'].values.reshape(-1, 1)\n",
        "undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=448)\n",
        "X_resampled, y_resampled = undersampler.fit_resample(x_reshape, df['n_votes_bin'])\n",
        "print(X_resampled)\n",
        "print(len(X_resampled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb87eT6h96AZ"
      },
      "outputs": [],
      "source": [
        "# commented out for now but theis is where the other feature vectors are created\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# # Sample DataFrame\n",
        "# df = pd.DataFrame(df)\n",
        "\n",
        "# analyzer = SentimentIntensityAnalyzer()\n",
        "# df[\"review_sentiment\"] = df[\"review_text\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
        "\n",
        "# # Create feature vector\n",
        "# feature_vector = df[['time_difference_days', 'review_word_count', 'num_reviews', 'rating', 'review_sentiment', 'num_book']]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(feature_vector, df['n_votes_bin'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe9pXjVQXtsE"
      },
      "outputs": [],
      "source": [
        "# sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=448)\n",
        "# X_train_idxs, y_train_idxs= sss.split(df['review_text'], df['n_votes_bin'])\n",
        "# print(X_train_idxs)\n",
        "# print(y_train_idxs)\n",
        "# print(X_test_idxs)\n",
        "# print(y_test_idxs)\n",
        "\n",
        "# splitter=StratifiedShuffleSplit(n_splits=1,random_state=12)\n",
        "\n",
        "# for train,test in splitter.split(df['review_text'], df['n_votes_bin']):     #this will splits the index\n",
        "#     X_train_SS = df['review_text'].iloc[train]\n",
        "#     y_train_SS = df['n_votes_bin'].iloc[train]\n",
        "#     X_test_SS = df['review_text'].iloc[test]\n",
        "#     y_test_SS = df['n_votes_bin'].iloc[test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj9c0OMKEXJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbfe5bc-9a1c-459e-9188-607843decb15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "n_votes_bin\n",
              "0    2742\n",
              "4    2742\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "df_bert = pd.DataFrame({'review_text': X_resampled.flatten(), 'n_votes_bin': y_resampled})\n",
        "df_bert['n_votes_bin'].value_counts()\n",
        "# df_bert = df[['review_text', 'n_votes_bin']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZoQgCm2_5qJ"
      },
      "source": [
        "## DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ4CTDALIlwk"
      },
      "outputs": [],
      "source": [
        "class MultiLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.review_text\n",
        "        self.targets = self.data.n_votes_bin\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "      # TODO: Return the number of the text inputs\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=False,\n",
        "            truncation = True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "\n",
        "        if self.targets[index] == 0:\n",
        "          new_label = 0\n",
        "        elif self.targets[index] == 4:\n",
        "          new_label = 1\n",
        "        else:\n",
        "          print('Unexpected label', self.targets[index])\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(new_label, dtype=torch.int)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCaP6RvAX1GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f613ca-98f8-4078-ef52-8e3986cfcc4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "       ..\n",
              "5479    4\n",
              "5480    4\n",
              "5481    4\n",
              "5482    4\n",
              "5483    4\n",
              "Name: n_votes_bin, Length: 5484, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "df_bert.n_votes_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI_rRmUaI-lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aeb920-2be1-4441-feb5-b16b31eba9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5484, 2)\n",
            "(4387, 3)\n",
            "(1097, 3)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(df_bert.shape)\n",
        "df_bert, valid_df = train_test_split(df_bert, test_size=0.2, random_state=448)\n",
        "\n",
        "df_bert = df_bert.reset_index()\n",
        "valid_df = valid_df.reset_index()\n",
        "\n",
        "print(df_bert.shape)\n",
        "print(valid_df.shape)\n",
        "\n",
        "training_set = MultiLabelDataset(df_bert, tokenizer, max_len=400)\n",
        "valid_set = MultiLabelDataset(valid_df, tokenizer, max_len=400)\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_size=4, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_set, batch_size=4, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObODazJXHk0H",
        "outputId": "efb00c49-5e24-4271-f37a-51c29a428148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      index                                        review_text  n_votes_bin\n",
            "0      1793  review originally posted books bottles accurat...            0\n",
            "1      1583  bleak post-apocalyptic story man son land almo...            0\n",
            "2       767  volume kicks chapter 80 clean-up 'll write edi...            0\n",
            "3       663  short book quaint really still something 's gr...            0\n",
            "4      4541  sigh read book 2008 hype highest point enjoy p...            4\n",
            "...     ...                                                ...          ...\n",
            "4382   1585  country somewhere south america birthday party...            0\n",
            "4383     78                    deserve 'm blinded tears review            0\n",
            "4384   2305      leo gursky top-five fictional characters time            0\n",
            "4385   2300  awsome book came book accident searching anoth...            0\n",
            "4386   4060  bear lot thoughts 've thought reading book yea...            4\n",
            "\n",
            "[4387 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beEFIz_RKK48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfde03b-c48b-4a0a-bbc4-710b726b8e73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBERTClass(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activation): ReLU()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc2): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (fc3): Linear(in_features=384, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # freeze distilbert layers\n",
        "        for param in self.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(768, 768)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(p = 0.2)\n",
        "        self.fc2 = torch.nn.Linear(768, 384)\n",
        "        # change number of bins\n",
        "        self.fc3 = torch.nn.Linear(384, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        self.hidden = self.distilbert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)[0]\n",
        "        x = self.hidden[:, 0, :]\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isqpEOX6NHx8"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFmLwBy_KN_6"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  num_epochs = 5\n",
        "  train_loss_history = []\n",
        "  train_acc_history = []\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_data in training_loader:\n",
        "    # for i, batch_data in tqdm(enumerate(training_loader), total=len(training_loader)):\n",
        "      # is this how to get the inputs on the same device, taken from lab\n",
        "      input_ids = batch_data['ids'].to(device)\n",
        "      attention_mask = batch_data['mask'].to(device)\n",
        "      targets = batch_data['targets'].type(torch.LongTensor).to(device)\n",
        "      #print(input_ids)\n",
        "      #print(targets)\n",
        "      #batch_data = batch_data.to(device)\n",
        "      #batch_data = batch_data.cuda()\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      #print(outputs.dtype)\n",
        "      #print(targets)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      # print(f\"Loss: {loss}\")\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Stats\n",
        "      total_loss += loss.item()\n",
        "      total += targets.size(0)\n",
        "      _, prediction = torch.max(outputs.data, 1)\n",
        "      # print(prediction)\n",
        "      correct += (prediction == targets).sum().item()\n",
        "\n",
        "    print(f\"Total Loss: {total_loss/total}\")\n",
        "    print(f\"Total Accuracy: {100*(correct/total)}\")\n",
        "    train_loss_history.append(total_loss/total)\n",
        "    train_acc_history.append(100*(correct/total))\n",
        "  return train_loss_history, train_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P_92ENhKOjQ",
        "outputId": "a93039a7-9b12-4fbd-e042-ebcc66650636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Total Loss: 2.9013571221914702\n",
            "Total Accuracy: 50.64964668338272\n",
            "Epoch: 1\n",
            "Total Loss: 0.2549424804246097\n",
            "Total Accuracy: 50.011397310234784\n",
            "Epoch: 2\n",
            "Total Loss: 0.1782572355918741\n",
            "Total Accuracy: 50.03419193070435\n",
            "Epoch: 3\n",
            "Total Loss: 0.17624403087620533\n",
            "Total Accuracy: 50.05698655117392\n",
            "Epoch: 4\n",
            "Total Loss: 0.1868391643291675\n",
            "Total Accuracy: 49.988602689765216\n"
          ]
        }
      ],
      "source": [
        "# CUDA_LAUNCH_BLOCKING=1\n",
        "tlh, tah = train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6UvhWXHWqIR"
      },
      "outputs": [],
      "source": [
        "# def validation(testing_loader):\n",
        "#     model.eval()\n",
        "#     pred_list = []\n",
        "#     actual_list = []\n",
        "#     with torch.no_grad():\n",
        "#       for batch_data in testing_loader:\n",
        "#         input_ids = batch_data['ids'].to(device)\n",
        "#         attention_mask = batch_data['mask'].to(device)\n",
        "#         actual = batch_data['targets'].to(device)\n",
        "\n",
        "#         pred = model(input_ids, attention_mask)\n",
        "#         pred_list.append(pred.cpu().numpy())\n",
        "#         actual_list.append(actual.cpu().numpy())\n",
        "\n",
        "#     return np.vstack(pred_list), np.vstack(actual_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uayMvM1Wo_8V"
      },
      "outputs": [],
      "source": [
        "def validation(testing_loader):\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    actual_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_data in testing_loader:\n",
        "            input_ids = batch_data['ids'].to(device)\n",
        "            attention_mask = batch_data['mask'].to(device)\n",
        "            actual = batch_data['targets'].to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            pred_list.append(logits.cpu().numpy())\n",
        "            actual_list.append(actual.cpu().numpy())\n",
        "    # for i in range(len(pred_list)):\n",
        "    #   print(f\"Prediction at index {i}: \")\n",
        "    #   print(pred_list[i])\n",
        "    #   print(np.shape(pred_list[i]))\n",
        "    # print(actual_list)\n",
        "    return np.concatenate(pred_list), np.concatenate(actual_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfKzBUDZWtsJ"
      },
      "outputs": [],
      "source": [
        "outputs, targets = validation(valid_loader)\n",
        "\n",
        "# TODO: Convert the output to match the labels (binarize)\n",
        "# final_outputs = (torch.sigmoid(torch.tensor(outputs)) > 0.5).cpu().numpy()\n",
        "# final_outputs = np.concatenate(outputs, axis=0)\n",
        "# final_outputs = (final_outputs > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-akcyq9j3r9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c303cb80-f476-4cc1-f479-8b454fc6942a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11468"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "len(df_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYDcmXMx1mi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c33745-59bb-4c04-bb2f-059ab22a3d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.11591198 -0.11919928]\n",
            " [ 0.11591198 -0.11919928]\n",
            " [ 0.11591198 -0.11919928]\n",
            " ...\n",
            " [ 0.11591198 -0.11919928]\n",
            " [ 0.11591198 -0.11919928]\n",
            " [ 0.11591198 -0.11919928]]\n"
          ]
        }
      ],
      "source": [
        "np.shape(outputs)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL13TQ7g8dya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3366fe6c-64bb-4de8-89d0-0e1a93e3a587"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "np.reshape(targets, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf6r_42O8fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3549bb-23e8-4e75-b49e-d51ca5cf15a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.11591198, -0.11919928],\n",
              "       [ 0.11591198, -0.11919928],\n",
              "       [ 0.11591198, -0.11919928],\n",
              "       ...,\n",
              "       [ 0.11591198, -0.11919928],\n",
              "       [ 0.11591198, -0.11919928],\n",
              "       [ 0.11591198, -0.11919928]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "np.argmax(outputs, axis=1)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA2YaNPi196X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "e0f43a1b-1fc2-441c-dde9-614d38d5ea8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[557 540]\n",
            " [  0   0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# np.sum(np.argmax(outputs, axis=1) ==\n",
        "# np.shape(np.reshape(targets, -1))\n",
        "np.sum(np.argmax(outputs, axis=1) == np.reshape(targets, -1))\n",
        "\n",
        "conf_mat = metrics.confusion_matrix(np.argmax(outputs, axis=1), np.reshape(targets, -1))\n",
        "print(conf_mat)\n",
        "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [0, 1, 2, 3, 4])\n",
        "sns.heatmap(conf_mat)\n",
        "# cm_display.plot()\n",
        "# plt.show()\n",
        "\n",
        "# actual raw outputs to see if truly learning 0 and 4\n",
        "# look at loss scores\n",
        "# change model\n",
        "\n",
        "\n",
        "# stratified splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S2QVLneqGq9"
      },
      "outputs": [],
      "source": [
        "# # Load the data\n",
        "# #df = pd.read_csv(GOOGLE_DRIVE_PATH + \"Combined_Goodreads_Data.csv\")\n",
        "\n",
        "# # Filter out instances where n_votes is 0\n",
        "# df_filtered = df[df['n_votes'] != 0]\n",
        "\n",
        "# # Preprocess the data\n",
        "# df_preprocessed = PreprocessData(df_filtered, nltk_stopwords)\n",
        "\n",
        "# # Create the feature vector for the entire preprocessed dataset\n",
        "# feature_vector = df_preprocessed[['time_difference_days', 'review_word_count', 'num_reviews', 'rating', 'review_sentiment', 'num_book']]\n",
        "\n",
        "# # Split the dataset into training and validation sets\n",
        "# train_df, valid_df = train_test_split(df_preprocessed, test_size=0.2, random_state=448)\n",
        "\n",
        "# # Create the feature vector for the validation set\n",
        "# valid_feature_vector = valid_df[['time_difference_days', 'review_word_count', 'num_reviews', 'rating', 'review_sentiment', 'num_book']]\n",
        "\n",
        "# # Ensure the indices are aligned\n",
        "# valid_feature_vector.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # Perform validation\n",
        "# valid_outputs, valid_targets = validation(valid_loader)\n",
        "\n",
        "# # Concatenate the logits with the feature vector for the validation set\n",
        "# combo_df_valid = np.concatenate([valid_outputs, valid_feature_vector], axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3ytHZBxzm0s"
      },
      "outputs": [],
      "source": [
        "#The hamming score is used to measure the similarity between two sets of binary data by calculates the number of correctly matched elements.\n",
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7itROkRGznEl"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate the hamming score between the real and actual values\n",
        "val_hamming_score = hamming_score(targets, final_outputs, normalize=True, sample_weight=None)\n",
        "\n",
        "print(f\"Hamming Score = {val_hamming_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c85G-XVhXu-l"
      },
      "outputs": [],
      "source": [
        "targets[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "443VHB7_zotQ"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate the per class accuracy\n",
        "# correct = np.sum(targets == final_outputs[:targets.shape[0]], axis=0)\n",
        "# accuracy_per_class = correct / targets.shape[0]\n",
        "# print(f\"Per Class Accuracy = {accuracy_per_class}\")\n",
        "confusion_matrix = metrics.multilabel_confusion_matrix(final_outputs, targets)\n",
        "for conf_mat in confusion_matrix:\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [False, True])\n",
        "  cm_display.plot()\n",
        "  plt.show()\n",
        "  # confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-orqF6vGpU0"
      },
      "outputs": [],
      "source": [
        "print(np.sum(final_outputs))\n",
        "# print(targets[0:10])\n",
        "\n",
        "# no True was predictred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKkAd4SpLLfu"
      },
      "source": [
        "Adding the other features to the BERT features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHxWakZZNQZ8"
      },
      "outputs": [],
      "source": [
        "# need to concatenate the last hidden state of BERT together with the feature vector\n",
        "# I am uncertain of the shape of the BERT output so this might have to be changed around but I think it is (N, 768)\n",
        "# https://www.geeksforgeeks.org/how-to-join-tensors-in-pytorch/\n",
        "# combo_df = torch.cat([outputs.last_hidden_state, feature_vector], -1)\n",
        "model.hidden"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}